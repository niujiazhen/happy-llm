import torch
import torch.nn as nn
import torch.nn.functional as F

import ModelConfig


# LLaMa2的MLP采用三个线性层和一个SILU激活函数
# 其中三个线性层采用了SwiGLU
class MLP(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):
        super().__init__()
        # 如果没有指定隐藏层的维度，我们将其设置为输入维度的4倍
        # 然后将其减少到2/3，最后确保它是multiple_of的倍数
        if hidden_dim is None:
            hidden_dim = 4 * dim
            hidden_dim = int(2 * hidden_dim / 3)
            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)
        # 定义第一层线性变换，从输入维度到隐藏维度
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        # 定义第二层线性变换，从隐藏维度到输入维度
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
        # 定义第三层线性变换，从输入维度到隐藏维度
        self.w3 = nn.Linear(dim, hidden_dim, bias=False)
        # 定义dropout层，用于防止过拟合
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # 前向传播函数
        # 首先，输入x通过第一层线性变换和SILU激活函数
        # 然后，结果乘以输入x通过第三层线性变换的结果
        # 最后，通过第二层线性变换和dropout层

        # SwiGLU激活函数的计算方式是：SwiGLU(x) = SiLU(w1(x)) * w3(x)，其中w1和w3是两个线性变换
        # 最后通过w2进行线性变换，并应用dropout
        # (Linear(x) * 激活(Linear(x))) → Linear
        # 一条分支做门控
        # 一条分支做特征
        # 相乘后增强表达能力

        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))
    

if __name__ == "__main__":
    args = ModelConfig.ModelConfig()
    # 创建MLP实例
    mlp = MLP(args.dim, args.hidden_dim, args.multiple_of, args.dropout)
    # 随机生成数据
    x = torch.randn(1, 50, args.dim)
    # 运行MLP模型
    output = mlp(x)
    print(output.shape)